# -*- coding: utf-8 -*-
"""Breast_Cancer_Pred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nTlI4qXBZ5HTtHvmCe1fVSjXep4Dmp6c
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/breastcancer.csv')

df.head()

"""option 2 to load files:
from google.colab import files
data= files.upload()

option 3:
directly from kaggle syntax
"""

#EDA
#checking the total no. of rows and columns
df.shape

df.info()

df.isna().sum()

df= df.dropna(axis=1)

df.shape

df.dtypes

#data vislz
df['diagnosis'].value_counts()

sns.countplot(data=df, x='diagnosis')
plt.show()

#encoding
from sklearn.preprocessing import LabelEncoder
labelencoder= LabelEncoder()

#transforming categorical to numerical
df.iloc[:,1]= labelencoder.fit_transform(df.iloc[:,1].values)

df.iloc[:,1].values

sns.pairplot(df.iloc[:,1:7],hue='diagnosis')
#to plot pairwise relationships between variables within a dataset.

#for correlations between columns
df.iloc[:,1:11].corr()



#heatmap
plt.figure(figsize= (10,10))
sns.heatmap(df.iloc[:,1:11].corr(),annot=True,fmt='.0%')

#Feature Scaling/ Modeling data
#split our dataset into independent (X) and dependent data sets (Y)
X= df.iloc[:,2:31].values
Y= df.iloc[:,1].values

#80:20 ratio
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=0.20, random_state=0)

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
X_train = sc.fit_transform(X_train)
X_test= sc.fit_transform(X_test)

X_train

#Logistic Regression classifier- supervised classf algo- used for predicting categorical dependent variable using A given set of independent variables.
#only 2 possible outcomes
#gives probablistic values btw 0 and 1
#disadv-- works only if the predicted variable is binary and also it assumes that the predictors are independent of each other.
def models(X_train, Y_train):
  from sklearn.linear_model import LogisticRegression
  log= LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)

  #Decision tree classifier- both classf and regr but used for classf
  #structure- top-down recursive divide and conquer approach
  #entropy- amount of uncertainity in the dataset
  from sklearn.tree import DecisionTreeClassifier
  tree= DecisionTreeClassifier(criterion='entropy', random_state=0)
  tree.fit(X_train, Y_train)

  #Random Forest classifier- no. of dec trees, takes average and improves accuracy of prediction
  # concept of ensemble learning- process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.
  from sklearn.ensemble import RandomForestClassifier
  forest= RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)
  forest.fit(X_train, Y_train)

  #accuracy of each model on training dataset
  print("The accuracy of Logistic Regression: ",log.score(X_train, Y_train))
  print("The accuracy of Decision tree classifier: ",tree.score(X_train, Y_train))
  print("The accuracy of Random forest classifier: ",forest.score(X_train, Y_train))

  return log,tree,forest

model=models(X_train,Y_train)

#CONFUSION MATRIX- describe perfm of classf model
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(Y_test, model[0].predict(X_test))
tp= cm[0][0]
tn= cm[0][1]
fn= cm[1][0]
fp= cm[1][1]
print(cm)
print('Accuracy: ',(tp+tn)/(tp+tn+fp+fn))

#Model accuracy on CM
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model: ',i+1)
  print(classification_report(Y_test, model[i].predict(X_test)))
  print(accuracy_score(Y_test, model[i].predict(X_test)))
  print()

#Model prediction vs Actual Prediction
pred= model[2].predict(X_test)
print('Our model prediction: ')
print(pred)
print()
print('Actual prediction: ')
print(Y_test)

